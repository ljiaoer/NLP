{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from jieba import analyse\n",
    "from gensim import corpora,models\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载停用词表\n",
    "def get_stopword_list():\n",
    "    stopword_path = \"./data/stopword.txt\"\n",
    "    stopword_list = [sw.replace(\"\\n\",\"\") for sw in open(stop_word_path,encoding = \"utf-8\").readlines()]\n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词\n",
    "def seg_to_list(sentence,pos=False):\n",
    "    if not pos:\n",
    "        #不进行词性标注的分词方法\n",
    "        seg_list=jieba.cut(sentence)\n",
    "    \n",
    "    else:\n",
    "        #进行词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除干扰词\n",
    "def word_filter(seg_list,pos=False):\n",
    "    stopword_list = get_stopword_list() \n",
    "    filter_list = []\n",
    "    #根据POS参数选择是否词性过滤\n",
    "    # 不进行词性过滤，则将词性都标记为n，表示为全部保留\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "            flag = \"n\"\n",
    "        else:\n",
    "            word = seg.word\n",
    "            flag = seg.flag\n",
    "        if not flag.startswith(\"n\"):\n",
    "            conitune\n",
    "        #过滤高停用词表中的词，以及长度小于2的词\n",
    "        if not word in stopword_list and len(word)>1:\n",
    "            filter_list.append(word)\n",
    "            \n",
    "    return filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集,并进行清洗。pos 为是否词性标注\n",
    "def load_data(pos = False,corpus_path =\"./data/corpus.txt\"):\n",
    "    #对数据集进行处理\n",
    "    doc_list = []\n",
    "    for line in open(corpus_path,\"r\"):\n",
    "        content = line.strip()\n",
    "        seg_list = seg_to_list(content,pos)\n",
    "        filter_list = word_filter(seg_list,pos)\n",
    "        doc_list.append(filter_list)\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
